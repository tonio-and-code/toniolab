#!/usr/bin/env python3
"""
å²©ï¨‘å’Œç”·AI - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
  python create-training-dataset.py

å…¥åŠ›: claudedocs/iwasaki-ai-training-data/*.md
å‡ºåŠ›: training_data/iwasaki_dataset.jsonl
"""

import os
import json
import re
from pathlib import Path

def extract_qa_pairs(md_content):
    """
    ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‹ã‚‰è³ªå•-å›žç­”ãƒšã‚¢ã‚’æŠ½å‡º
    """
    pairs = []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯ â†’ è§£èª¬
    pattern1 = r'> \*\*ã€Œ(.+?ï¼‰ã€\*\*\n\n\*\*è§£èª¬\*\*:\n(.+?)(?=\n\n|$)'
    matches1 = re.findall(pattern1, md_content, re.DOTALL)
    for question, answer in matches1:
        pairs.append({
            "instruction": question.strip(),
            "output": answer.strip()
        })

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ### è¦‹å‡ºã— â†’ å†…å®¹
    pattern2 = r'### (.+?)\n(.+?)(?=\n###|$)'
    matches2 = re.findall(pattern2, md_content, re.DOTALL)
    for title, content in matches2:
        # é•·ã™ãŽã‚‹å†…å®¹ã¯æœ€åˆã®æ®µè½ã ã‘
        content_clean = content.split('\n\n')[0].strip()
        if len(content_clean) > 50 and len(content_clean) < 500:
            pairs.append({
                "instruction": f"{title}ã«ã¤ã„ã¦æ•™ãˆã¦",
                "output": content_clean
            })

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã€è³ªå•ã€‘â†’ã€å›žç­”ã€‘å½¢å¼
    pattern3 = r'\*\*è³ªå•\*\*:\s*(.+?)\n\*\*å›žç­”\*\*:\s*(.+?)(?=\n\n|$)'
    matches3 = re.findall(pattern3, md_content, re.DOTALL)
    for question, answer in matches3:
        pairs.append({
            "instruction": question.strip(),
            "output": answer.strip()
        })

    return pairs

def create_alpaca_format(pairs):
    """
    Alpacaå½¢å¼ã«å¤‰æ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¨™æº–å½¢å¼ï¼‰
    """
    alpaca_data = []
    for pair in pairs:
        alpaca_data.append({
            "instruction": pair["instruction"],
            "input": "",
            "output": pair["output"]
        })
    return alpaca_data

def main():
    # ãƒ‘ã‚¹è¨­å®š
    input_dir = Path("claudedocs/iwasaki-ai-training-data")
    output_dir = Path("training_data")
    output_dir.mkdir(exist_ok=True)

    all_pairs = []

    # å…¨MDãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    for md_file in input_dir.glob("*.md"):
        print(f"å‡¦ç†ä¸­: {md_file.name}")

        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()

        pairs = extract_qa_pairs(content)
        all_pairs.extend(pairs)
        print(f"  â†’ {len(pairs)}ä»¶ã®QAãƒšã‚¢ã‚’æŠ½å‡º")

    # Alpacaå½¢å¼ã«å¤‰æ›
    alpaca_data = create_alpaca_format(all_pairs)

    # JSONLå½¢å¼ã§ä¿å­˜ï¼ˆ1è¡Œ1JSONï¼‰
    output_file = output_dir / "iwasaki_dataset.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in alpaca_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"\nâœ… å®Œæˆ: {output_file}")
    print(f"ðŸ“Š åˆè¨ˆ {len(alpaca_data)}ä»¶ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿")

    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("\nã€ã‚µãƒ³ãƒ—ãƒ«ã€‘")
    for i, item in enumerate(alpaca_data[:3], 1):
        print(f"\n{i}. è³ªå•: {item['instruction'][:50]}...")
        print(f"   å›žç­”: {item['output'][:100]}...")

if __name__ == "__main__":
    main()
